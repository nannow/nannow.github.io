---
layout: post
title: Machine Learning Week 7 - (1)
comments: false
categories: [ML]
tags: [ML]
---

<br><br>
<span style="color:red">[이 자료는 [Coursera](https://www.coursera.org/) Stranford University Andrew Ng 교수님의 강의 자료를 활용하였습니다.]</span>
<br><br>


<hr><br>
## * Optimization Objective
<br>
이번에 소개할 SVM은 복잡한 비선형 함수를 학습하는데 있어서 로지스틱 회귀나 뉴럴네트워크보다 더욱 강력하게 작용할 수 있는 하나의 학습 방법입니다.
<br><br>
<div align="center">
<img width="550" alt="1" src="https://user-images.githubusercontent.com/17719651/44843825-35054b00-ac84-11e8-81d3-e24fc93545d9.png">
</div>
<br><br>
SVM은 코스트 부분만 보자면 로지스틱 회귀와 비슷한 모양을 가집니다. 로지스틱 회귀는 z값에 따라 로그함수 그래프를 그대로 따라 그리며 값을 갖지만 SVM은 z값에 따라 코스트를 두 개의 직선으로 나눕니다. <br>y=1일 때, z값은 1을 기준으로 y=0일 때, z값은 -1을 기준으로 그래프가 나뉘는데 이것에 대해서는 다음에 좀 더 자세히 알아보겠습니다.
<br><br>
<div align="center">
<img width="550" alt="2" src="https://user-images.githubusercontent.com/17719651/44843826-35054b00-ac84-11e8-9268-bae2b07de6b9.png">
</div>
<br><br>
이제 SVM을 어떻게 수식으로 표현할 지 알아보겠습니다. 로지스틱 회귀 모델과 비슷한 식을 갖지만 상수곱 부분이 조금 다릅니다. <br>로지스틱 회귀에서 A + lambda * B 모양을 가졌던 코스트 함수 모양은 SVM에서 C * A + B 모양으로 표현합니다. 여기서 C는 람다의 역수값 입니다. 어차피 목적은 코스트 함수를 최소로 하게 하는 theta 값을 찾는것이라서 각 식 앞에 붙은 상수곱은 최적의 theta 값을 찾는데 영향을 주지 않습니다. 그것에 대한 예시로 이차함수에서 최소값 u값 찾기를 예시로 들었네요.<br>
로지스틱 회귀와 SVM의 상수곱 차이가 어떤 의미를 갖는지는 깊게 들어가지 않았지만 대략적인 이유는 A와 B식 사이의 trade-off 때문이라고 합니다.
<br><br>
<div align="center">
<img width="550" alt="3" src="https://user-images.githubusercontent.com/17719651/44843827-359de180-ac84-11e8-8c93-114ad8d265e0.png">
</div>
SVM의 코스트 함수식을 보여줍니다. 익숙한 모양의 식이라 아직은 어려운 점이 없는것 같네요.
<br>
<br>
## * Large Margin Intuition
<br>
<div align="center">
<img width="550" alt="1" src="https://user-images.githubusercontent.com/17719651/44849329-bb298d80-ac94-11e8-9f74-e3fde4f7101e.png"></div>
<br><br>
SVM의 코스트 함수식과 가설 그래프가 그려져 있습니다. 로지스틱 회귀와는 다르게 z값이 0을 기준으로 나누는것이 아니라 y=1일 때 z >= 1 , y=0일 때 z <= -1 을 만족시키도록 만들고 싶습니다. 그리고 이러한 조건은 SVM을 Large margin classifier 라고 부르게 만듭니다. 아래 그림을 살펴보겠습니다.
<br><br>
<div align="center">
<img width="550" alt="2" src="https://user-images.githubusercontent.com/17719651/44849330-bb298d80-ac94-11e8-994c-d72c579ceba1.png">
</div>
<br>
SVM에서 선형적으로 positive 와 negative 를 나눌 떄 일반적으로 검은 직선처럼 decision boundary 를 그리게 됩니다. 검은 선을 기준으로 positive 케이스와도 어느정도 거리가 있고 negative 케이스와도 어느정도 거리가 있어 보입니다. 그리고 이 각각의 거리를 margin 이라고 부릅니다. <br>
초록색선과 분홍색선은 margin 이 없는 엄격한 구분선 처럼 보이는데 이러한 선은 선형회귀에서 쉽게 볼 수있는 decision boundary 처럼 보입니다.
<br><br>
<div align="center">
<img width="550" alt="3" src="https://user-images.githubusercontent.com/17719651/44849331-bb298d80-ac94-11e8-992c-5e6d718409d9.png">
</div><br>
SVM의 코스트 함수식은 C * A + B 의 모양을 갖는다고 했습니다. C가 람다의 역수값인건 알고 있는데 C의 역할은 무엇일까요?<br>
C의 값이 무한히 커지면 A는 0에 수렴하게 될 것입니다. 이것은 positvie 와 negative 를 나누는 기준이 보다 엄격해지는것을 의미합니다. <br>
C의 값이 작으면 positive 와 negative 를 나눌 때 large margin 을 갖게 되고 postive 영역에 negative 가 들어가거나 혹은 반대의 상황이 와도 기준에 대해 관대해집니다.<br>그림을 보면 positive 바로 아래에 negative 가 존재하지만 C의 값이 작을 때 검은색 선을 유지함을 확인할 수 있습니다.
<br>

<br><br>
<hr>
<br>

<br><br><br><br>
