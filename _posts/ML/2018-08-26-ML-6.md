---
layout: post
title: Machine Learning Week 5 - (1)
comments: false
categories: [ML]
tags: [ML]
---

<br><br>
<span style="color:red">[이 자료는 [Coursera](https://www.coursera.org/) Stranford University Andrew Ng 교수님의 강의 자료를 활용하였습니다.]</span>
<br><br>



<br>
## * Cost Function
<br>
뉴럴 네트워크의 코스트 함수에 대해 알아보겠습니다.<br><br>
<img width="705" alt="1" src="https://user-images.githubusercontent.com/17719651/44471764-8c456300-a667-11e8-864c-de1b6e1d124a.png"><br>
<br>
위 그림에서 Training set, L, s_l 에 대한 용어 정의가 되어있고 이진 분류 문제와 다중 분류 문제에 대한 비교가 나와있습니다. 이진 분류 문제는 당연히 바이너리 값이기 때문에 output이 1차원 실수값으로 나오면 되지만 다중 분류 문제는 K차원의 벡터로 output이 나와야 합니다.
<br><br>
<img width="696" alt="2" src="https://user-images.githubusercontent.com/17719651/44471765-8c456300-a667-11e8-8437-0c113b894960.png"><br><br>
정규화 과정이 포함된 로지스틱 회귀의 코스트 함수를 약간만 변형시켜주면 뉴럴 네트워크의 코스트 함수식이 나옵니다. 시그마가 여러개 붙어서 복잡해보이지만  가설값이 1차원 실수에서 K차원의 벡터로 확장되었다는 점과 theta 가 열벡터 였는데 행렬로 확장되었다는 점을 고려하면 이론적으로 복잡한 내용은 없는것 같습니다.
<br><br>



## * Backpropagation Algorithm
<br>
Backpropagation Algorithm (역전파 알고리즘) 은 코스트 함수를 최소화 시키는 뉴럴 네트워크의 용어 중 하나입니다. 이번에는 코스트 함수를 편미분 하는 방정식에 대해서 알아보겠습니다. <br>
<br>
<img width="624" alt="1" src="https://user-images.githubusercontent.com/17719651/44479567-ecde9b00-a67b-11e8-981d-146651755366.png"><br>
위 그림은 역전파 알고리즘을 보기 전에 순전파의 표기법에 대해서 한번 더 익히는 그림입니다.<br>
<br>
<img width="622" alt="2" src="https://user-images.githubusercontent.com/17719651/44479568-ecde9b00-a67b-11e8-98cf-22539765d0eb.png"><br><br>
델타의 표기법을 한번 확인하고 무엇을 의미하는지 보겠습니다. 델타의 superscript는 몇번째 레이어인지 나타내고 subscript는 레이어의 몇번째 유닛인지를 나타냅니다. 델타는 액티베이션 유닛 a와 training set의 실제값인 y의 차로 정의됩니다. 델타를 l번째 레이어의 j번째 오류 노드라고 표현하는 이유도 바로 이것입니다. 위에서 델타가 수식을 통해서 어떻게 표현되는지도 확인할 수 있습니다. .* 연산자 같은 경우는 matlab에서 쓰이는데 엘리먼트끼리 각각 곱해주는 연산자입니다.
<br><br>
<img width="646" alt="3" src="https://user-images.githubusercontent.com/17719651/44479569-ecde9b00-a67b-11e8-8101-b6b24c7bf909.png"><br><br>
역전파 알고리즘은 다음과 같습니다. 처음에 m개의 training set이 주어지고 대문자 델타값을 0으로 초기화 합니다. 순전파를 수행한 마지막 레이어인 L번째 레이어에서 소문자 델타값을 계산합니다. 이제 순전파가 왔던 방향의 반대로 레이어를 이동하면서 델타2까지 계산하고 소문자 델타들을 업데이트 시킵니다. training set이 m개 있기 때문에 m번의 루프를 돌려줍니다. 그림의 가장 아래부분에 대문자 D는 코스트 함수를 weight matrix로 편미분한 값입니다. 이전에 선형회귀와 로지스틱회귀에서 했던 gradient descent 방법과 비슷해 보입니다. 
<br><br>

## * Backpropagation Intuition
<br>
이제 역전파 알고리즘에 대한 직관적인 이해가 필요할 것 같습니다.
<br>
<img width="625" alt="4" src="https://user-images.githubusercontent.com/17719651/44558137-79bd4d80-a77d-11e8-98c0-7a6cfbeff60c.png"><br>
<br>
우선 순전파에 대해 다시 생각해 보겠습니다. 각 레이어의 bias 들을 +1로 설정해주고 각 유닛들의 표기법을 다시 한번 확인해 봅니다. 그 다음 3번째 레이어의 첫번째 z값이 어떻게 표기되야 하는지 확인해 줍니다.
<br><br>
<img width="631" alt="5" src="https://user-images.githubusercontent.com/17719651/44558138-7a55e400-a77d-11e8-92f7-20de735fa7d2.png"><br><br>
역전파의 코스트 함수 표기법에서 training set 중 하나의 데이터 쌍인 (x,y)에 대해 표기한 식이 있습니다. 복잡한 수식을 피하기 위해서 우선 정규화 텀인 lambda 텀은 제외시키고 마지막 레이어의 output이 오직 1개만 존재한다고 가정해보겠습니다. 코스트 함수의 코스트 부분을 따로 정의해주고 다음번에 무엇을 해야할 지 생각해봤을 때 이런 의문을 갖게 됩니다. 이 뉴럴 네트워크는 i번째 데이터 쌍에 대해 얼마만큼의 오차를 갖게 될까? 이 질문에 답을 하기 전에 표기법에 대해 한번 더 익숙해지도록 아래 그림을 보겠습니다. 
<br><br>
<img width="636" alt="6" src="https://user-images.githubusercontent.com/17719651/44558139-7a55e400-a77d-11e8-8237-189ba245d67d.png"><br><br>
이전까지 순전파와 역전파에 다루면서 델타에 대해 여러번 언급했습니다. 델타는 실제 y값과 액티베이션 유닛인 a와의 차이값이라고 했습니다. 위 그림에서 2번째 레이어의 2번째 유닛에 있는 델타값을 구하기 위해서 세번째 레이어의 델타값들을 필요하다는 걸 확인했습니다. 우선적으로 순전파를 실행한 후 역전파를 실행해야 델타값을 구할 수 있다는것도 이미 알고있는 사실입니다. 델타는 l번째 레이어의 j번째 유닛에 있는 에러값이라고 알고 있었지만 정형적으로는 우리가 새로 정의한 코스트 표기법을 z에 대해 편미분한 값이 바로 델타였던 것입니다. 우리가 이 델타를 이용하여 이미 역전파 알고리즘의 코스트값을 구하는 법을 알고 있기 때문에 우선 여기까지 이해했다면 뉴럴 네트워크 코드에는 어떻게 적용되어야 하는지 다음 시간에 알아보겠습니다.
<br>








<br><br>

<br><br><br><br>














































