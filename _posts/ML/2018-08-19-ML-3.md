---
layout: post
title: Machine Learning Week 3 - (1)
comments: false
categories: [ML]
tags: [ML]
---

<br><br><br><br>

## Classification

분류(Classification) 문제는 우리가 예측하길 바라는 value 값이 작은 이산적인 값을 다룬다는 점을 제외하면 regression model 과 비슷하다고 할 수 있습니다. 지금은 binary 분류 문제만을 다룰것입니다. (0과 1만 존재) 예를들면, 이메일 스팸 분류기를 만든다고 하면 y=1 은 스팸이고 y=0 은 스팸이 아니라고 할 수 있습니다.
<br>

<img width="678" alt="1" src="https://user-images.githubusercontent.com/17719651/44150472-8b515554-a0da-11e8-8d7d-a031244f6abb.png">
<br><br>
## Hypothesis Representation
<br>
분류 문제에서는 y=1과 y=0인 두 가지 경우가 있다고 했는데 그렇다면 기계학습 예측값인 h(x) 함수가 1보다 크고 0보다 작은 값을 결과로 낸다는 것은 적절치 않아 보입니다. 이런 문제를 해결하기 위해서 이전에 가지고 있던 h 함수를 0과 1사이의 값을 갖도록 식을 변형해야 합니다. θTx (theta transpose x) 값을 Logistic function에 대입해보겠습니다. 아래의 식과 같은 형태를 Sigmoid function 또는 Logistic function 이라고 합니다.
<br>
<br>
<img width="610" alt="2" src="https://user-images.githubusercontent.com/17719651/44150473-8b84a274-a0da-11e8-8499-cc64f3682101.png">
<br>
<img width="610" alt="3" src="https://user-images.githubusercontent.com/17719651/44150474-8bb17588-a0da-11e8-8f36-dc7575c090f0.png">
<br>
<br><br>
위에서 보다시피 h()x = g(z)이고 이 함수는 0과 1사이에 있는 실수값을 갖게 됩니다. h(x) 함수는 output이 1이 되는 확률값을 반환합니다. 예를들면, h(x) = 0.7 이 식은 output이 1이 될 확률이 70% 라는것을 의미합니다. 분류 문제에서는 output이 0과 1만 존재하므로 아래 수식이 성립하게 됩니다.

<img width="610" alt="4" src="https://user-images.githubusercontent.com/17719651/44150475-8c626640-a0da-11e8-82eb-dfc37e6ee487.png">
<br>
<br>

## Decision Boundary

<br>
<img width="585" alt="11" src="https://user-images.githubusercontent.com/17719651/44254475-e3010600-a23d-11e8-9741-6277222c9f4e.png"><br> 
<img width="576" alt="12" src="https://user-images.githubusercontent.com/17719651/44254477-e3999c80-a23d-11e8-92e2-920d76aa9bb5.png"><br><br>
위의 h(x) = g(z) 함수 식을 보면 theta 열 벡터가 [-3,1,1] 이라고 정해져 있습니다. theta 열 벡터를 transpose 하고 x의 값과 내적하면 z 값이 나오고 그 z값이 0보다 클 때 y=1이라고 예측 해보겠습니다. (sigmoid 또는 logistic function에서는 z값이 0보다 커야 output이 1이라고 가정했기 때문) 그러면 x1과 x2 두 변수가 있는 일차 함수가 나오고 decision boundary를 x1-x2평면에 그릴 수 있습니다. linear한 모델이므로 매우 간단해보입니다.<br>
<img width="601" alt="13" src="https://user-images.githubusercontent.com/17719651/44254478-e3999c80-a23d-11e8-9409-b468ba191b14.png"><br>
<br><br>
위와 같이 non-linear한 케이스에는 어떻게 decision boundary를 만들 수 있을까요? Machine learning week 2 에서 봤던 polynomial regression 을 이용해보면 간단하게 표현할 수 있습니다. theta벡터와 x벡터가 반드시 linear 함을 보장하지 않기 때문에 위의 그림과 같은 다항식으로 표현해주면 원 모양의 decision boundary도 만들 수 있습니다. 

<br>

## Cost Function
<br>
분류 문제에서의 Cost function에 대해 알아보겠습니다.
<br>
<img width="648" alt="1" src="https://user-images.githubusercontent.com/17719651/44258122-02516080-a249-11e8-9171-64fca4e8114f.png">
<br><br>
위 그림에서 training set, n개의 feature, hypothesis 함수가 주어져 있습니다. 여기까진 좋은데 어떻게 theta 벡터를 구할 수 있을까요? <br><br>
<img width="651" alt="2" src="https://user-images.githubusercontent.com/17719651/44258123-02e9f700-a249-11e8-9a23-a5f554686022.png">
<br><br>
Linear regression 에서는 h함수와 실제값 차이의 제곱을 시그마로 더하고 training set의 개수만큼 나눠준 값으로 코스트 함수의 값을 결정할 수 있었습니다. 하지만 linear regression 에서 사용했던 코스트 함수 모델을 logistic regression 모델에 그대로 적용한다면 위의 그림 왼쪽과 같은 non-convex한 그래프 모양이 나오고 미분했을 때 theta가 0이되는 local-minimum이 많이 존재할 수 있으므로 global-minimum을 찾기가 어려워 집니다. 우리가 원하는 것은 convex한 코스트 함수의 모양이고 global-minimum을 쉽게 찾을 수 있어야 합니다.<br><br>
<img width="698" alt="3" src="https://user-images.githubusercontent.com/17719651/44258124-03828d80-a249-11e8-9f0f-560429530ced.png">
<br><br>
<img width="688" alt="4" src="https://user-images.githubusercontent.com/17719651/44258126-041b2400-a249-11e8-8b42-f286b44a5dd1.png">
<br><br>
위의 공식은 logistic regression 을 위한 새로운 코스트 함수식 입니다. y=1일 때와 y=0일 때 2가지 케이스로 나눠서 그래프를 그리게 됩니다. 두 가지 케이스에서 코스트 값이 왜 커지는 지는 가설값과 실제값을 비교해보면 알 수 있겠네요.<br><br>
<br>

## Simplified Cost Function and Gradient Descent

<br>
<img width="631" alt="1" src="https://user-images.githubusercontent.com/17719651/44301213-f3080b00-a34d-11e8-9ade-ca6dbce53a9e.png"><br>
Logistic regression 에서 코스트 함수를 정의할 때 y=1일 때와 y=0일 때를 나누어 정의했습니다. 어차피 분류 문제에서 y=0,y=1 이 두 경우 밖에 없기 때문에 이 식을 하나로 합쳐서 정의하면 위와 같은 식이 나오게 됩니다.<br>
<br>
<img width="630" alt="2" src="https://user-images.githubusercontent.com/17719651/44301214-f3080b00-a34d-11e8-951c-17f4cf9986ef.png"><br>
코스트 함수를 구했으니 다음에 해야할 일은 코스트 함수의 값을 최소화 시킬 수 있는 매개변수 theta 들을 찾는 것 입니다. <br>
<br>
<img width="615" alt="3" src="https://user-images.githubusercontent.com/17719651/44301215-f4393800-a34d-11e8-9266-8319770c7ba4.png"><br>
<img width="631" alt="4" src="https://user-images.githubusercontent.com/17719651/44301216-f4393800-a34d-11e8-9794-d62ccfc93812.png"><br>
이전에 linear regression 에서 했던 Gradient descent 와 동일한 과정을 거칩니다. hypothesis 함수를 정의해주고 코스트 함수를 정의한 다음 코스트 함수의 값을 최소화 시킬 수 있는 theta 벡터를 찾습니다. 각 theta 는 각각 업데이트 하는것이 아니라 동시에 업데이트 되어야 합니다. 
<br>
<br>

<br><br><br><br>














































