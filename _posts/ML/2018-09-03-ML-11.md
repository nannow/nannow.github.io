---
layout: post
title: Machine Learning Week 7 - (2)
comments: false
categories: [ML]
tags: [ML]
---

<br><br>
<span style="color:red">[이 자료는 [Coursera](https://www.coursera.org/) Stranford University Andrew Ng 교수님의 강의 자료를 활용하였습니다.]</span>
<br><br>


<hr><br>
## * Kernels 1
<br>
지금까지는 Non-linear 한 Decision boundary 를 그리기 위해 polynomial 그래프를 그렸습니다. polynomial 가설 함수 를 다항식으로 표현하고 계산하는것은 굉장히 많은 연산과정을 요구합니다.(computationally expensive) <br> 그래서 Non-linear 한 Decision boundary 를 그릴 수 있는 다른 방법이 있는지 찾아보고자 합니다.
<br>
<div align="center">
<img width="557" alt="1" src="https://user-images.githubusercontent.com/17719651/44963996-198e8e80-af69-11e8-99c1-d448a25fdef4.png">
</div>
<br>
위 그림은 가설함수의 다항식 표현에서 theta 부분을 제외한 나머지 부분을 각각 f로 나타내고 있습니다. polynomial 가설함수에서는 이 f 부분이 feature 들의 곱으로 나타나지만 이 f가 다르게 표현되면서 좀 더 좋게 나타나질 수 있는 다른 방법이 있을까요?
<br>

<div align="center">
<img width="558" alt="2" src="https://user-images.githubusercontent.com/17719651/44963997-1a272500-af69-11e8-839d-4ce7cce68ca8.png">
</div><br>
위 물음에 대한 대답 중 하나는 바로 Kernel 을 사용하는 것입니다. kernel 중에서 가장 대표적인 kernel 이 바로 Gaussian Kernal 입니다. 위 그림에서는 l1, l2, l3 세 가지의 커널을 가지고 있습니다. <br> <br>f를 표현하기 위해서 similarity 라는 함수를 사용하는데 그 함수는 가우시안 분포를 따르고 있습니다. similarity 에는 두 개의 파라미터가 필요한데 하나는 학습 셋인 x가 들어가고 다른 하나는 우리가 지정할 커널 중 하나인 l이 들어갑니다.
<br><br>
<div align="center">
<img width="551" alt="3" src="https://user-images.githubusercontent.com/17719651/44963998-1a272500-af69-11e8-9ff9-12efc87e4556.png">
</div>
similarity 함수의 두 파라미터인 x와 l이 근사하면 f는 1에 가까워지고 x와 l이 근사하지 않으면 f는 0의 값에 가까워진다는 것을 확인할 수 있습니다. 이것이 무슨 의미인지 아래에서 좀 더 알아보겠습니다.
<br>
<div align="center">
<img width="552" alt="4" src="https://user-images.githubusercontent.com/17719651/44963999-1a272500-af69-11e8-8944-7064f9c870c4.png">
</div>
<br><br>
위 그림에서는 시그마의 제곱에 따라 그래프 모양이 달라지는것을 확인할 수 있는데 시그마의 제곱이 커질수록 가우스 분포 모양의 그래프가 넓게 퍼지는것을 알 수 있고, 시그마의 제곱이 작을수록 좁게 퍼지는것을 확인할 수 있습니다.
<br>
<div align="center">
<img width="552" alt="5" src="https://user-images.githubusercontent.com/17719651/44964000-1abfbb80-af69-11e8-9561-36ef8d5b0bc0.png">
</div>
<br><br>
최적의 theta 들을 구했고 커널들이 위의 그림과 같이 배치되있다고 가정해보겠습니다. 임의의 x 가 어느 커널들과 근접해있는지 확인하면 f 의 값을 쉽게 구할 수 있습니다. 그래서 구한 f 값과 구해져있는 theta 들을 계산해서 0보다 크면 '1' 이라고 예측하게 됩니다.
<br>
<br><br>
## * Kernels 2
<br>
<div align="center">
<img width="556" alt="6" src="https://user-images.githubusercontent.com/17719651/44964064-8bff6e80-af69-11e8-9655-9254424705c0.png">
</div>
<br><br>
이전까지는 커널이 어디쯤에 위치해있다고 가정하고 문제에 접근했지만 이제는 이 커널을 어떻게 구할지에 대해 알아보겠습니다.
<br>
<br>
<div align="center">
<img width="555" alt="7" src="https://user-images.githubusercontent.com/17719651/44964065-8bff6e80-af69-11e8-94e6-6f6379f2ca7a.png">
</div>
<br><br>
m 개의 데이터 셋이 주어졌을 때, 우리가 구하고 싶던 커널들의 정체는 각각의 m 개의 데이터 중 입력값인 x 라는 사실을 확인할 수 있습니다. 표기법이 조금 헷갈리지만 각각의 학습 셋에 대응하는 f 값이 존재한다는것을 확인할 수 있습니다. i 번째 학습 데이터에 대해서 i 번째 f 벡터를 만들 수 있습니다.
<br><br>
<div align="center">
<img width="556" alt="8" src="https://user-images.githubusercontent.com/17719651/44964066-8c980500-af69-11e8-999f-85daae0e0147.png">
</div>
<br><br>
커널을 이용한 SVM 구현에 대해 보겠습니다. theta 를 최적화 시키기 위해서 학습 셋의 입력값인 x 대신에 f 를 넣은것을 확인할 수 있습니다. 이전까지는 정규화 텀에서 theta 의 제곱을 구할 때 theta transpose * theta 로 간단하게 구했었습니다. <br><br>SVM 에서는 theta transpose * M 행렬 * theta 로 정규화 텀을 구한다고 합니다. M 행렬은 직접 자신이 임의의 값을 가진 행렬을 정해주는 것이 아니라 SVM 소프트웨어 패키지들이 처리해준다고 합니다. 그리고 그렇게 사용하는것이 본인이 직접 짠 코드보다 훨씬 효율적이라는것을 강조하고 있습니다.
<br><br>
<div align="center">
<img width="558" alt="9" src="https://user-images.githubusercontent.com/17719651/44964067-8c980500-af69-11e8-8155-f2dbaa203687.png">
</div>
<br><br>
SVM 의 C 값의 의미에 대해 알아보겠습니다. C 값이 크면 high variance (오버피팅) 가 되고, C 값이 작으면 high bias (언더피팅) 경향이 강해집니다.<br> <br> 시그마 제곱이 커지면 가우스 분포 모양의 그래프가 넓게 퍼지므로 어떤 특정 구간에 데이터 쏠림이 방지된다고 할 수 있습니다. 그래서 언더피팅 경향을 갖습니다. 반대로 시그마 제곱이 작아지면 좁은 모양의 그래프를 갖으면서 오버피팅 경향이 강해집니다.
<br>

<br><br>
<hr>
<br>

<br><br><br><br>
