---
layout: post
title: Machine Learning Week 5 - (1)
comments: false
categories: [ML]
tags: [ML]
---

<br><br>
<span style="color:red">[이 자료는 [Coursera](https://www.coursera.org/) Stranford University Andrew Ng 교수님의 강의 자료를 활용하였습니다.]</span>
<br><br>




## * Cost Function
<br>
뉴럴 네트워크의 코스트 함수에 대해 알아보겠습니다.
<img width="705" alt="1" src="https://user-images.githubusercontent.com/17719651/44471764-8c456300-a667-11e8-864c-de1b6e1d124a.png"><br>
<br>
위 그림에서 Training set, L, s_l 에 대한 용어 정의가 되어있고 이진 분류 문제와 다중 분류 문제에 대한 비교가 나와있습니다. 이진 분류 문제는 당연히 바이너리 값이기 때문에 output이 1차원 실수값으로 나오면 되지만 다중 분류 문제는 K차원의 벡터로 output이 나와야 합니다. 
<img width="696" alt="2" src="https://user-images.githubusercontent.com/17719651/44471765-8c456300-a667-11e8-8437-0c113b894960.png"><br>
정규화 과정이 포함된 로지스틱 회귀의 코스트 함수를 약간만 변형시켜주면 뉴럴 네트워크의 코스트 함수식이 나옵니다. 시그마가 여러개 붙어서 복잡해보이지만  가설값이 1차원 실수에서 K차원의 벡터로 확장되었다는 점과 theta 가 열벡터 였는데 행렬로 확장되었다는 점을 고려하면 이론적으로 복잡한 내용은 없는것 같습니다.
<br>



## * Backpropagation Algorithm
<br>
Backpropagation Algorithm (역전파 알고리즘) 은 코스트 함수를 최소화 시키는 뉴럴 네트워크의 용어 중 하나입니다. 이번에는 코스트 함수를 편미분 하는 방정식에 대해서 알아보겠습니다. 
<br>
<img width="624" alt="1" src="https://user-images.githubusercontent.com/17719651/44479567-ecde9b00-a67b-11e8-981d-146651755366.png"><br>
위 그림은 역전파 알고리즘을 보기 전에 순전파의 표기법에 대해서 한번 더 익히는 그림입니다.
<br>
<img width="622" alt="2" src="https://user-images.githubusercontent.com/17719651/44479568-ecde9b00-a67b-11e8-98cf-22539765d0eb.png"><br>
델타의 표기법을 한번 확인하고 무엇을 의미하는지 보겠습니다. 델타의 superscript는 몇번째 레이어인지 나타내고 subscript는 레이어의 몇번째 유닛인지를 나타냅니다. 델타는 액티베이션 유닛 a와 training set의 실제값인 y의 차로 정의됩니다. 델타를 l번째 레이어의 j번째 오류 노드라고 표현하는 이유도 바로 이것입니다. 위에서 델타가 수식을 통해서 어떻게 표현되는지도 확인할 수 있습니다. .* 연산자 같은 경우는 matlab에서 쓰이는데 엘리먼트끼리 각각 곱해주는 연산자입니다.
<br>
<img width="646" alt="3" src="https://user-images.githubusercontent.com/17719651/44479569-ecde9b00-a67b-11e8-8101-b6b24c7bf909.png"><br>
역전파 알고리즘은 다음과 같습니다. 처음에 m개의 training set이 주어지고 대문자 델타값을 0으로 초기화 합니다. 순전파를 수행한 마지막 레이어인 L번째 레이어에서 소문자 델타값을 계산합니다. 이제 순전파가 왔던 방향의 반대로 레이어를 이동하면서 델타2까지 계산하고 소문자 델타들을 업데이트 시킵니다. training set이 m개 있기 때문에 m번의 루프를 돌려줍니다. 그림의 가장 아래부분에 대문자 D는 코스트 함수를 weight matrix로 편미분한 값입니다. 이전에 선형회귀와 로지스틱회귀에서 했던 gradient descent 방법과 비슷해 보입니다. 
<br>

## * Backpropagation Intuition
<br>



<img width="625" alt="4" src="https://user-images.githubusercontent.com/17719651/44558137-79bd4d80-a77d-11e8-98c0-7a6cfbeff60c.png"><br>
<br>
<img width="631" alt="5" src="https://user-images.githubusercontent.com/17719651/44558138-7a55e400-a77d-11e8-92f7-20de735fa7d2.png"><br>
<br>
<img width="636" alt="6" src="https://user-images.githubusercontent.com/17719651/44558139-7a55e400-a77d-11e8-8237-189ba245d67d.png"><br>
<br>













<br><br>

<br><br><br><br>














































