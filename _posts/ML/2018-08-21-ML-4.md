---
layout: post
title: Machine Learning Week 3 - (2)
comments: false
categories: [ML]
tags: [ML]
---

<br><br><br><br>


## Multiclass Classfication: One vs all
<br>
이번엔 다중 분류 문제에 대해 알아보도록 하겠습니다. 이전까지 본 기본적인 분류 문제는 y=0일 때와 y=1일 때 2가지로 나누게 되지만 다중 분류 문제는 y=0 y=1 y=2 ... y=n 까지 n+1개의 카테고리 분류를 다루게 됩니다. 아래 사진을 보면 분류 문제가 어떻게 확장되어야 하는지에 대해 알 수 있습니다.
<br>
<img width="643" alt="5" src="https://user-images.githubusercontent.com/17719651/44302037-6b75c880-a35c-11e8-89d4-ea3b869d65d1.png"><br>
<img width="613" alt="6" src="https://user-images.githubusercontent.com/17719651/44302038-6b75c880-a35c-11e8-82f1-2308797a9e34.png"><br> 
<br>
<img width="851" alt="7" src="https://user-images.githubusercontent.com/17719651/44302039-6b75c880-a35c-11e8-811a-e99ae48c5d37.png"><br>
<br>
다중 분류 문제는 기본적인 이중 분류 문제를 각각의 케이스에 대해 반복적으로 시행한 후 max값을 갖는 hypothesis 를 사용하게 됩니다. 
<br>
<br>

## The Problem of Overfitting
<br>
머신러닝에서 주어진 training data set으로 신뢰할만한 가설 모델을 만들고 싶습니다. 그랬을 때 새로운 데이터가 들어오고 우리가 예상했던 결과값과 머신러닝을 통해 예측한 값이 일치할 확률을 좀 더 높이면 좋겠습니다. 이러한 과정에서 데이터의 Overfitting 과 Underfitting이 발생할 수 있습니다.
<img width="653" alt="8" src="https://user-images.githubusercontent.com/17719651/44313771-cc7aca80-a448-11e8-9d99-f07e2621e8e1.png"><br>
<br>
위에 그림에서 보듯이 training set이 있고 그 data set에 너무 많은 feature를 가지게 되면 학습된 가설 모델은 training set에 오차가 거의 나지않는 방향으로 만들어질 수 있습니다. 주어진 training set에만 맞추는 것이 목표가 아니라 새로운 데이터가 들어왔을 때 그 데이터에 대한 판별도 정확성을 갖어야 하기 때문에 우리는 좀 더 일반적인 가설 모델을 원하게 됩니다.

<img width="651" alt="9" src="https://user-images.githubusercontent.com/17719651/44313773-cd136100-a448-11e8-94c2-6ed875a0f0fd.png"><br>
위 그림은 언더 피팅,적절한 피팅,오버 피팅 세 가지에 대한 다른 예시 입니다.
<br>
<br>
<img width="644" alt="10" src="https://user-images.githubusercontent.com/17719651/44313774-cd136100-a448-11e8-9c9c-ed82970e0d18.png"><br>
<br>
오버 피팅을 피하기 위해서 2가지 방법이 있습니다. 첫번째는 feature의 개수를 줄이는 것이고 두번째는 정규화 시키는 것입니다. 모델 선택 알고리즘과 정규화에 대해서는 나중에 다룰것입니다.
<br>
<br>
## Cost Function
<br>
오버피팅을 막기 위한 방법 중 한가지로 Regularization(정규화)가 있습니다. 아래 그림을 통해 정규화가 어떻게 이루어지는지 알아보겠습니다.
<br>
<img width="608" alt="1" src="https://user-images.githubusercontent.com/17719651/44314870-dd333c80-a458-11e8-8fea-80589a8b0c7c.png"><br>
정규화 과정을 거치는 코스트 함수식은 두 가지 텀을 가지게 됩니다. 우리가 처음 배웠던 코스트 함수 식 + 람다가 곱해진 시그마 식 이렇게 두 가지 텀을 가집니다.<br>
<br>
<img width="597" alt="2" src="https://user-images.githubusercontent.com/17719651/44314871-dd333c80-a458-11e8-866f-842fa23a947c.png"><br>
코스트 함수식을 최소화 시키려고 할 때, 람다가 곱해진 theta^2 들의 합 값 또한 최소화 되어야 하기 때문에 전체적인 theta들의 값은 정규화 과정을 거치지 않은 기본적인 코스트 함수식의 theta들 보다 더 작은 값을 갖게 됩니다. 그 말은 training data set을 통해 만들어진 예측 모델이 데이터에 맞게 오버피팅 되는것이 아니라 보다 일반적인 그래프 모양을 갖게됨을 의미합니다.
<br><br>
<img width="608" alt="3" src="https://user-images.githubusercontent.com/17719651/44314872-dd333c80-a458-11e8-9b0f-7cc9b6e9134f.png"><br><br>
위의 그림은 정규화 매개변수인 람다값이 매우 커지면 어떻게 되는지에 대한 질문입니다. 람다값이 매우커지면 theta들의 값은 매우 작아질것입니다. 각각의 theta들이 0으로 수렴하면 가설함수는 theta0 값에 수렴하게 되고 결국은 상수함수가 되어 언더피팅이 된다는 내용입니다. <br>
<br>
<br>



## Regularized Linear Regression
<br>
정규화된 선형회귀에 대해서 알아보겠습니다. 일반적인 선형회귀 모델과는 달리 람다곱이 들어간 텀이 추가로 들어가는 것을 이전에 확인하였습니다. 
<img width="573" alt="1" src="https://user-images.githubusercontent.com/17719651/44403058-65672e00-a58e-11e8-80fe-7a73ce24a8f8.png"><br>
위 그림과 같이 정규화된 선형회귀 코스트 함수를 표현할 수 있고 이 코스트 함수값을 최소화 시키는 theta 벡터를 찾는것이 목적입니다. 
<br>
<img width="597" alt="2" src="https://user-images.githubusercontent.com/17719651/44403059-65ffc480-a58e-11e8-8efe-aee7fdac3c61.png"><br><br>
Gradient descent 과정에서 편의를 위해 theta0인 부분과 j=1부터 j=n까지의 부분을 따로 나눠 줍니다. 정규화 과정에서는 theta j 를 업데이트 할 때마다 theta j 값이 줄어들어야 하므로 1-a*lamda/m 부분은 1보다 작아야 합니다. 
<br>
<img width="586" alt="3" src="https://user-images.githubusercontent.com/17719651/44403061-6730f180-a58e-11e8-975e-6330a1ba07ff.png"><br><br>
Normal equation 의 기본 아이디어는 Gradient descent 에서 했던 각 theta들을 업데이트 시키는 것이 아니라 코스트 함수의 미분을 통해서 최적의 theta 들을 찾고 global-minimum을 얻는 것이었습니다. 유도 과정을 생략한 채 공식을 적은 것이 위의 그림에 나와있습니다. 어려운 점은 없지만 연산을 할 때 주의할 점은 벡터나 행렬들의 사이즈를 잘 고려해야 한다는 점 입니다. 
<br>


<br>
## Regularized Logistic Regression
<br>

<img width="621" alt="4" src="https://user-images.githubusercontent.com/17719651/44404896-f6400880-a592-11e8-9bca-1dd942f4ed9b.png"><br>
정규화된 선형회귀와 크게 다른점이 없어 보입니다. 다만 이름 그대로 선형회귀는 일차원적인 회귀를 다루는데 비해서 로지스틱 회귀는 고차원적인 회귀까지 다루기 때문에 위의 그래프와 같은 모양을 만들어낼 수 있습니다.
<br><br>
<img width="629" alt="5" src="https://user-images.githubusercontent.com/17719651/44404897-f6400880-a592-11e8-9d11-ea15a0692831.png"><br>
Gradient descent 에서도 선형회귀 모델과 같은 공식을 사용하면 됩니다. 로지스틱 회귀 이기 때문에 가설 함수 모양이 조금 다르다는 점만 주의하면 될것 같습니다.
<br>
<br>











<br><br><br><br>














































