---
layout: post
title: Machine Learning Week 1
comments: false
categories: [ML]
tags: [ML]
---

<br><br><br><br>


## Supervised Learning


Supervised learning 에서는, 데이터 set이 주어지고 correct output이 어떻게 나와야 하는지 이미 알고 있는 상태입니다.
input과 output 사이의 관계도 어느정도 알 수 있는 상태입니다. Supervised learning 에서는 "regression" 과 "classfication" 
두 분류로 나눌 수 있습니다. regression 에서는 연속적인 output 내에서 결과를 예측하려고 합니다. 즉, input 변수를 연속 함수에
매핑하려는 시도를 합니다. classification 에서는 이산적인 output 내에서 결과를 예측하려고 합니다. input 변수를 이산적인 카테고리에 매핑한다는 의미입니다.

**예제 1** : 
부동산 시장에서 집들의 크기에 관한 데이터가 주어진다면, 그것들의 가격에 대해 예측하려는 시도를 해봅시다. 가격은 연속적인 output 이므로 regression 문제라고 할 수 있습니다.

하지만, 위의 문제를 원하는 가격보다 더 값 싸게 혹은 비싸게 팔 수 있는지에 대한 결과(Yes or No)는 classfication 문제로 바뀌어 질 수 있습니다.



**예제 2** :

(a) : Regression : 사람의 사진이 주어졌을 때, 주어진 사진에 근거하여 나이를 추측할 수 있다. <br>
(b) : Classfication : 종양을 가진 환자가 있을 때, 종양이 음성인지 양성인지 판단할 수 있다. (True or false)


<br>
## Unsupervised Learning


Unsupervised learning 은 우리의 결과가 어떻게 보일지에 대해 전혀 모르는 상태로 문제에 접근하게 됩니다. 데이터 내에 변수들 사이에
관계에 근거하여 데이터를 clustering(군집) 함으로써 구조를 얻을 수 있습니다. 이러한 unsupervised learning 은 예측결과에 근거한
feedback이 없습니다.


**예제**:

Clustering : 1,000,000 가지의 유전자를 모으고 이들 유전자를 수명, 위치, 역할 등과 같은 다양한 변수를 통해 얼마나 유사하고 관련이 있는지를 자동으로 분류해줍니다.

Non-clustering : Cocktail Party Algorithm 은 복잡한 환경에서 구조를 찾을 수 있습니다.(혼잡한 사운드에서 개별적인 음성과 음악을 분류 할 수 있습니다.)


<br>

## Model Representation

<br>
<br>

<img width="399" alt="2018-08-14 6 42 11" src="https://user-images.githubusercontent.com/17719651/44060339-55669806-9f8f-11e8-881c-f41a20973452.png">

x(i) : Input variables <br>
y(i) : Output variables <br>
(x(i),y(i)) : training example <br>

우리의 목표는 supervised learning 문제를 좀 더 정형적으로 기술하는 것이고, h:X->Y라는 함수를 배우기 위해 training set가
주어집니다. h(x)는 y의 해당 값에 대한 일종의 "좋은" 판단 척도가 됩니다. h함수는 hypothesis 라고도 불립니다. 
<br><br>

## Cost Function
<br>
Cost function(코스트 함수)를 이용함으로써 hypothesis 함수의 정확성을 측정할 수 있습니다. 예측 value와 실제 value의 차들의 제곱의 평균을 통해서 구할 수 있습니다. 아래의 그림을 통해 코스트 함수가 어떻게 작용하는지를 알 수 있습니다.

<img width="362" alt="2018-08-14 6 42 19" src="https://user-images.githubusercontent.com/17719651/44060341-558acbd6-9f8f-11e8-97b9-9d7f6367ae59.png">

<br>

<img width="652" alt="2018-08-14 6 42 23" src="https://user-images.githubusercontent.com/17719651/44060342-55b0e636-9f8f-11e8-9340-bb7ec6dd6516.png">

<br><br>
## Gradient Descent
<br>
여태까지 hypothesis 함수와 그 함수가 얼마나 데이터에 잘 맞는지 알아보았습니다. 이제 hypothesis 함수내에 있는 매개변수들을 측정할 차례입니다. θ0를 x축에 θ1을 y축에 넣고, 코스트 함수를 z축으로 잡아줍니다.

<img width="570" alt="2018-08-14 6 41 35" src="https://user-images.githubusercontent.com/17719651/44060337-551743c8-9f8f-11e8-9f64-1586ca3b39c9.png">
<br>
<br>
위의 그림에서 빨간색으로 동그라미 표시된 곳이 initial point이고 빨간 화살표가 가리키는 지점이 local minimum 입니다. 목표는 initial point를 정했을 때, local minimum 까지 불필요한 연산이 없이 효율적으로 갈 수 있는가? 입니다. 이러한 과정을 위해 코스트 함수의 미분을 사용할 것입니다. 식은 아래와 같고 local-minimum 에 도착할 때 까지 (즉, 수렴할 때 까지) 아래의 식의 값을 업데이트하며 반복할 것입니다. 코스트함수 미분의 계수부분에 있는 알파값은 learning rate 라고 불리는데 이것에 대해선 나중에 다뤄볼 것 입니다
<br>
<img width="230" alt="2018-08-14 6 43 17" src="https://user-images.githubusercontent.com/17719651/44060343-55d4e964-9f8f-11e8-80a9-f8f62d88ba94.png">
<br>

여기서 주의 해야할 점은,  모든 매개변수들 θ1,θ2,...θn 은 동시에 업데이트 되어야 한다는 것입니다. 아래 그림을 보면 초기화와 계산이 어떤식으로 이루어져야 하는지 알 수 있습니다.
<br>
<br>
<img width="661" alt="2018-08-14 6 41 47" src="https://user-images.githubusercontent.com/17719651/44060338-553fe7c4-9f8f-11e8-8920-ced93077110e.png">
<br>
<br>
<br><br><br>
