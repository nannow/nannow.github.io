---
layout: post
title: Deep Convolutional Generative Adversarial Network (DCGAN) - (3)
comments: false
categories: [Pytorch]
tags: [Pytorch]
---

#### 소스코드 원본 링크 :  [https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)

## Training 

### Part 1 - Train the Discriminator

판별자 학습의 목표는 '진짜' 또는 '가짜' 입력을 올바르게 구분할 확률을 최대화하는 것이다. 실질적으로 우리는 log(D(x)) + log(1-D(G(z))) 를 최대화하길 원한다. mini-batch 방식을 사용하기 때문에 우리는 이러한 것을 두 단계로 계산할 것이다.<br><br>

첫 번째로 학습 set에서 '진짜' 샘플 배치를 구성하고 이 배치를 D를 통해 순방향 전달하고 손실을 계산 한 다음 역방향 전달에서 그레디언트를 계산한다.<br><br>

두 번째로 현재 생성자의 '가짜' 샘플 배치를 구성하고 이 배치를 D를 통해 순방향 전달하고 손실 log(1-D(G(z))) 를 계산하고 역방향 전달에서 그레디언트를 누적한다. <br><br>

이제 모든 '진짜' 와 '가짜' 배치들로부터 축적된 그레디언트를 가지고 판별자의 optimizer step을 호출한다.

### Part 2 - Train the Generator

더 나은 '가짜' 를 생성하기 위해 log(1-D(G(z))) 를 최소화하여 생성자를 훈련시키려 한다. 이 값을 최소화하기 위해서 log(D(G(z)))를 최대화할 것이다. 

-   **Loss_D**  - discriminator loss calculated as the sum of losses for the all real and all fake batches (log(D(x))+log(D(G(z)))log(D(x))+log(D(G(z)))).
-   **Loss_G**  - generator loss calculated as  log(D(G(z)))log(D(G(z)))
-   **D(x)**  - the average output (across the batch) of the discriminator for the all real batch. This should start close to 1 then theoretically converge to 0.5 when G gets better. Think about why this is.
-   **D(G(z))**  - average discriminator outputs for the all fake batch. The first number is before D is updated and the second number is after D is updated. These numbers should start near 0 and converge to 0.5 as G gets better. Think about why this is.


```python
# Training Loop

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0

print("Starting Training Loop...")
# For each epoch
for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):

        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output, label)
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate batch of latent vectors
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)
        # Classify all fake batch with D
        output = netD(fake.detach()).view(-1)
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion(output, label)
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = netD(fake).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output, label)
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                  % (epoch, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())

        # Check how the generator is doing by saving G's output on fixed_noise
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

        iters += 1
```

## Results

```python
plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

<img width="575" alt="2019-01-17 11 18 28" src="https://user-images.githubusercontent.com/17719651/51325163-a87eec00-1aaf-11e9-8080-60b79284aefa.png">

```python
#%%capture
fig = plt.figure(figsize=(8,8))
plt.axis("off")
ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]
ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)

HTML(ani.to_jshtml())
```

<img width="612" alt="2019-01-17 11 28 05" src="https://user-images.githubusercontent.com/17719651/51325167-a9178280-1aaf-11e9-92e7-3b631c513872.png">

```python
# Grab a batch of real images from the dataloader
real_batch = next(iter(dataloader))

# Plot the real images
plt.figure(figsize=(15,15))
plt.subplot(1,2,1)
plt.axis("off")
plt.title("Real Images")
plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))

# Plot the fake images from the last epoch
plt.subplot(1,2,2)
plt.axis("off")
plt.title("Fake Images")
plt.imshow(np.transpose(img_list[-1],(1,2,0)))
plt.show()
```

<img width="639" alt="2019-01-17 11 28 21" src="https://user-images.githubusercontent.com/17719651/51325172-aa48af80-1aaf-11e9-989e-261e6f8e3495.png">

<br><br>
